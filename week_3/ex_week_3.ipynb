{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 3 - Convolutional neural networks\n",
    "\n",
    "## Overview\n",
    "*Also available (p. 287 - 321 in Bishop)*\n",
    "\n",
    "\n",
    "CNN‚Äôs are a class of Neural networks, typically used in:\n",
    "1. Image analysis: Since the amount of parameters they use is effectively invariant in\n",
    "regards to the size of the image. Moreoever because they can learn to do feature\n",
    "extraction much the same way regular convolution does.\n",
    "2. Time signal analysis: Again, since their number of parameters do not scale with the\n",
    "length of the observed signal. Also again since they can learn to mimic a lot of the\n",
    "convolutions (which is essentially filtering) that have proven themselves useful in\n",
    "the past\n",
    "\n",
    "Like every other neural network class out there, there are a million-and-one specific\n",
    "methods and ideas to improve their performance, of all these, these exercises only ex-\n",
    "plore pooling. Moreoever, there is a billion-and-two different specific architectures that\n",
    "are useful in specific or different cases. For this course, we will only examine a specific implementation of the \"basic\" CNN: VGG16-D.\n",
    "Even so, with these fundamentals in mind, you should be able to at least understand\n",
    "what it is most other implementations of these neural networks do.\n",
    "\n",
    "**Feel free to jump between the theoretical exercises as much as you like. Like always,\n",
    "be sure to ask plenty of ‚Äôstupid‚Äô questions; CNN‚Äôs probably more than any\n",
    "other subject, has many ‚ÄùI really should understand this, but I don‚Äôt, there-\n",
    "fore I must be dumb‚Äù-moments, so don‚Äôt sweat it if it seems trivial, it probably\n",
    "isn‚Äôt.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Convolutional Layers\n",
    "\n",
    "### Exercise 1.1 (ch. 10.2.5)\n",
    "\n",
    "Consider one convolution kernel for a CNN with the given parameters:\n",
    "- Size: $3 \\times 3$\n",
    "- Padding (zero-padded): $1$\n",
    "- Stride: $1$\n",
    "- Input channels: $3$\n",
    "\n",
    "****1:** How many (learnable) parameters does this kernel have in a CNN?**\n",
    "\n",
    "$\\dots$\n",
    "\n",
    "\n",
    "****2:** In general, how many parameters does an ùëõ √ó ùëõ kernel with d input channels, ùëù padding and ùë† stride have?** \n",
    "\n",
    "$\\dots$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.2\n",
    "\n",
    "Say you now have a convolutional layer with the following hyperparameters:\n",
    "\n",
    "- Kernel size: $n \\times n$\n",
    "- Input channels: $d$\n",
    "- Output channels: $k$\n",
    "\n",
    "\n",
    "**1: How many learnable parameters does this whole convolutional layer have?**\n",
    "\n",
    "$\\dots$\n",
    "\n",
    "\n",
    "**2: Check your manual calculation against that of the torch implementation below**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# Parameters for a simple convolutional layer\n",
    "in_channels = 3\n",
    "out_channels = 128\n",
    "kernel_size = (2,3) # Does not have to be a tuple, if int, will be converted to a square kernel\n",
    "stride = 1\n",
    "padding = 0\n",
    "bias = True\n",
    "\n",
    "# Define conv layer and sum number of parameters\n",
    "conv_layer = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride, padding=padding, bias=True)\n",
    "num_params = sum(p.numel() for p in conv_layer.parameters())\n",
    "\n",
    "# Calculate number of parameters manually\n",
    "num_params_manual = ...\n",
    "\n",
    "print(f\"Torch thinks there are {num_params} parameters in the layer layer \\nCompared to {num_params_manual} calculated manually\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.3\n",
    "\n",
    "Given a convolutional layer with the following hyperparameters:\n",
    "- Size: $n \\times n$\n",
    "- Padding: $p$\n",
    "- Stride: $s$\n",
    "- Input channels: $d$\n",
    "- Output channels: $k$\n",
    "\n",
    "\n",
    "**1. What is the dimensionality of the output? (Do not forget to include the number of\n",
    "channels)**\n",
    "\n",
    "$\\dots$\n",
    "\n",
    "\n",
    "**2. Likewise, what is the dimensionality after a pooling layer with the same parameters ?**\n",
    "\n",
    "$\\dots$\n",
    "\n",
    "**3. Check your answer against the torch code below**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL.Image\n",
    "import torchvision.transforms.functional\n",
    "\n",
    "# Once more, Parameters for a simple convolutional layer\n",
    "in_channels = 3\n",
    "out_channels = 128\n",
    "kernel_size = (2,3) # Does not have to be a tuple, if int, will be converted to a square kernel\n",
    "stride = 1\n",
    "padding = 0\n",
    "bias = True\n",
    "\n",
    "# Define conv layer and sum number of parameters\n",
    "conv_layer = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride, padding=padding, bias=True)\n",
    "\n",
    "# Define two dimensional max pooling layer\n",
    "max_pool_layer = nn.MaxPool2d(kernel_size=kernel_size, stride=stride, padding=padding)\n",
    "\n",
    "# Load image and convert to tensor\n",
    "a_cool_image = PIL.Image.open(\"a_cool_image.jpg\").convert(\"RGB\")\n",
    "# Unsqueeze to add batch dimension (because torch is nasty like that and expects a batch dimension)\n",
    "a_cool_image_tensor = torchvision.transforms.functional.to_tensor(a_cool_image).unsqueeze(0)\n",
    "\n",
    "print(f\"Image shape before convolution: {a_cool_image_tensor.shape}\")\n",
    "\n",
    "# Apply convolution and max pooling\n",
    "convolved_image = conv_layer(a_cool_image_tensor)\n",
    "pooled_image = max_pool_layer(a_cool_image_tensor)\n",
    "\n",
    "print(f\"Image shape after convolution: {convolved_image.shape}\")\n",
    "print(f\"Image shape after max pooling: {pooled_image.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.4\n",
    "\n",
    "*It should be clear now, that each channel in the output image, corresponds to one kernel\n",
    "being run across all channels in the input image. Therefore with $k$ output channels, we\n",
    "have to train $k$ kernels, each with $m \\times n \\times d + 1$ parameters.*\n",
    "\n",
    "**1. Discuss (and write down) the benefits of having a larger number of output channels\n",
    "compared to input channels.**\n",
    "\n",
    "$\\dots$\n",
    "\n",
    "**2. Increasing the number of output channels in one layer obviously leads to more ker-\n",
    "nels (and therefore more parameters), but how does this increased number of output\n",
    "channels affect the kernels in subsequent layers in a neural network?**\n",
    "\n",
    "$\\dots$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.5\n",
    "\n",
    "*If you read the torch documentation of [Conv2d](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html), you won‚Äôt find anywhere that\n",
    "it flips the kernel as is otherwise required to go from cross-correlation to convolution.*\n",
    "\n",
    "\n",
    "**1. Explain how PyTorch, a respectable deep learning framework, can get away with *not*\n",
    "flipping the kernel**\n",
    "\n",
    "$\\dots$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Exercise 1.6\n",
    "\n",
    "*As mentioned, one reason CNN‚Äôs are used for image processing, is because they don‚Äôt\n",
    "explode in parameter complexity as the input image increases in size. Here we will\n",
    "bastardize the big-O notation from Algorithms and Data structures to denote how the\n",
    "number of parameters in a nerual network increases as a function of relevant hyperparameters.*\n",
    "\n",
    "**1. In the aforementioned big-O, how does the number of parameters in a fully connected,\n",
    "feedforward neural network (FFN) grow as a function of image size (assume multiple\n",
    "input channels)?**\n",
    "\n",
    "$\\dots$\n",
    "\n",
    "**2. Again in the Big-O notation, how does the number of parameters in a CNN grow as a\n",
    "function of image size and other relevant layer hyperparameters?**\n",
    "\n",
    "$\\dots$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coding CNN's\n",
    "\n",
    "*Theres a **bunch** of boilerplate that goes into every NN implementation in Python. Our advice, is to understand and solve the exercises given first, and then look into this boilerplate. A lot of people and guides tell you to simply ignore this boilerplate code... **But it will haunt you***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.datasets as datasets\n",
    "from torch import nn\n",
    "from torchvision.transforms import ToTensor\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Finally, a useful function that we do not explicitly use, but you can check out if you want\n",
    "from torchsummary import summary # courtesy of https://stackoverflow.com/questions/55875279/how-to-get-an-output-dimension-for-each-layer-of-the-neural-network-in-pytorch\n",
    "\n",
    "\n",
    "# Check if you have cuda available, and use if you do\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a dataset, change 'dataset' to 'cifar10' if you want to use CIFAR-10 instead\n",
    "dataset = 'mnist'\n",
    "\n",
    "if dataset == 'cifar10':\n",
    "    train_set = datasets.CIFAR10(root='./data', train=True, download=True, transform=ToTensor())\n",
    "    test_set = datasets.CIFAR10(root='./data', train=False, download=True, transform=ToTensor())\n",
    "\n",
    "    # Purely for our convenience - Mapping from cifar labels to human readable classes\n",
    "    cifar10_classes = {\n",
    "        0: 'airplane',\n",
    "        1: 'automobile',\n",
    "        2: 'bird',\n",
    "        3: 'cat',\n",
    "        4: 'deer',\n",
    "        5: 'dog',\n",
    "        6: 'frog',\n",
    "        7: 'horse',\n",
    "        8: 'ship',\n",
    "        9: 'truck'\n",
    "    }\n",
    "\n",
    "elif dataset == 'mnist':\n",
    "    train_set = datasets.MNIST(root='./data', train=True, download=True, transform=ToTensor())\n",
    "    test_set = datasets.MNIST(root='./data', train=False, download=True, transform=ToTensor())\n",
    "\n",
    "print(f\"There are {len(train_set)} examples in the training set\")\n",
    "print(f\"There are {len(test_set)} examples in the test set \\n\")\n",
    "\n",
    "print(f\"Image shape is: {train_set[0][0].shape}, label example is {train_set[0][1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dataloaders - You don't *need* these per say, but they are very useful\n",
    "\n",
    "# Collate function is called on each batch, the dataloader yields\n",
    "# Here, we modify it to also cast the tensor to our desired device\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "def collate_fn(batch):\n",
    "    return tuple(x_.to(device) for x_ in default_collate(batch))\n",
    "\n",
    "# Why don't we shuffle the test set? Because we don't need to.\n",
    "train_dataloader = torch.utils.data.DataLoader(train_set, batch_size=16, shuffle=True, collate_fn=collate_fn)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_set, batch_size=16, shuffle=False, collate_fn=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show a few examples from the dataset\n",
    "\n",
    "*This is basically tradition at this point when making CNN introductions*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get indices of 10 different classes in the dataset, then get values\n",
    "sampled_indices = [np.random.choice(np.where(np.array(train_set.targets) == i)[0]) for i in range(10)]\n",
    "examples = train_set.data[sampled_indices]\n",
    "\n",
    "fig, axes = plt.subplots(2, 5, figsize=(12, 5))\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Plot examples\n",
    "for i in range(len(examples)):\n",
    "    img = examples[i]\n",
    "    if dataset == 'mnist':\n",
    "        img = img.squeeze(0)  # Remove the channel dimension for MNIST\n",
    "        axes[i].imshow(img, cmap='gray')\n",
    "        axes[i].set_title(f'Class {i}')\n",
    "    else:\n",
    "        axes[i].imshow(img)\n",
    "        axes[i].set_title(f'Class {i}:{cifar10_classes[train_set.targets[sampled_indices[i]]]}')\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a CNN class\n",
    "\n",
    "*Remember the following:\n",
    "*\n",
    "            \n",
    "- Remember here, that the amount of parameters in a convolutional layer is $(n * m * l + 1) * k$  \n",
    "   - *Where* $n, m$ *is the kernel size* $(x, y)$, $l$ *is the input channels*, *and* $k$ *is the* $\\text{output channels} + 1$, *is beacuse of a bias term that is done for each input*\n",
    "  - *Basically*, $n * m * l$ *corresponds to kernels mapping to values in another \"image\"*\n",
    "  - *Each kernel has a bias term unique to it. Each kernel produces one \"image\", that are then stacked on top of each other, for a total of* $k$ *images*.\n",
    "  - *CONV2D does NOT flip the kernel!!!*\n",
    "    - *But, it doesn't really mattter, since it is just learned anyways...*\n",
    "\n",
    "*Finally, I found an answer on if it is better to ReLU first or Max Pool first... it kinda doesn't matter*\n",
    "- *[But it is slightly better to maxpool first, max-pooling and monotonely increasing non-linearities commute. This means that MaxPool(Relu(x)) = Relu(MaxPool(x)) for any input](https://stackoverflow.com/questions/35543428/activation-function-after-pooling-layer-or-convolutional-layer\n",
    ") *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(torch.nn.Module):\n",
    "    def __init__(self, num_classes, in_channels=1, features_fore_linear=64*6*6, lr=0.001):\n",
    "        super().__init__()\n",
    "\n",
    "        # Define layers as a torch.nn.Sequential object\n",
    "        # This is pretty nice, since we can just go layers(input) to get output\n",
    "        # Rather than having a bunch of functions in the forward function\n",
    "        self.layers = torch.nn.Sequential(\n",
    "            nn.Conv2d(in_channels=in_channels, out_channels=32, kernel_size=3, padding=1), # dim = in\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(in_features=features_fore_linear, out_features=600),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=600, out_features=120),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=120, out_features=num_classes)\n",
    "        ).to(device)\n",
    "                \n",
    "        # Loss function and optimizer, as you know, Adam is meta\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.optim = torch.optim.Adam(self.layers.parameters(), lr=lr)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "    def train_model(self, train_dataloader, epochs=1, val_dataloader=None):\n",
    "        \n",
    "        # To hold accuracy during training and testing\n",
    "        train_accs = []\n",
    "        test_accs = []\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            \n",
    "            epoch_acc = 0\n",
    "\n",
    "            for inputs, targets in tqdm(train_dataloader):\n",
    "                logits = self(inputs)\n",
    "                loss = self.criterion(logits, targets)\n",
    "                loss.backward()\n",
    "\n",
    "                self.optim.step()\n",
    "                self.optim.zero_grad()\n",
    "\n",
    "                # Not actually used for training, just for keeping track of accuracy\n",
    "                epoch_acc += (torch.argmax(logits, dim=1) == targets).sum().item()\n",
    "\n",
    "            train_accs.append(epoch_acc / len(train_dataloader.dataset))\n",
    "\n",
    "            # If we have val dataloader, we can evaluate after each epoch\n",
    "            if val_dataloader is not None:\n",
    "                acc = self.eval_model(val_dataloader)\n",
    "                test_accs.append(acc)\n",
    "                print(f\"Epoch {epoch} validation accuracy: {acc}\")\n",
    "        \n",
    "        return train_accs, test_accs\n",
    "\n",
    "    def eval_model(self, test_dataloader):\n",
    "        \n",
    "        total_acc = 0\n",
    "\n",
    "        for input_batch, label_batch in test_dataloader:\n",
    "            # Get predictions\n",
    "            logits = self(input_batch)\n",
    "\n",
    "            # Remember, outs are probabilities (so there's 10 for each input)\n",
    "            # The classification the network wants to assign, must therefore be the probability with the larget value\n",
    "            # We find that using argmax (dim=1, because dim=0 would be across batch dimension)\n",
    "            classifications = torch.argmax(logits, dim=1)\n",
    "            total_acc += (classifications == label_batch).sum().item()\n",
    "\n",
    "        total_acc = total_acc / len(test_dataloader.dataset)\n",
    "\n",
    "        return total_acc\n",
    "    \n",
    "    def save_model(self, path_to_state_dict):\n",
    "        # TODO: Implement saving model\n",
    "        ...\n",
    "\n",
    "    def load_model(self, path_to_state_dict):\n",
    "        # TODO: Implement loading model\n",
    "        ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A small hack\n",
    "\n",
    "*Normally, you have to ask ChatGPT, a friend or the dark Gods to find out what the dimensionality of the data is before the first linear layer (why has Torch not implemented this automatically????) Anyways, the below function solves that, while still being a bit cursed*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dim_before_first_linear(layers, in_dim, in_channels, brain=False):\n",
    "    \"\"\"\n",
    "    Assume square in dimensions, square kernels, cuz I'm lazy\n",
    "    Also assume kernel numbers and channels match up, because that's trivial enough\n",
    "    \"\"\"\n",
    "\n",
    "    current_dim = in_dim\n",
    "    current_channels = in_channels\n",
    "    for layer in layers:\n",
    "        if isinstance(layer, nn.Conv2d) or isinstance(layer, nn.MaxPool2d):\n",
    "            # If the layer padding is same we do not need to change the dimension of the input\n",
    "            if layer.padding == 'same':\n",
    "                if isinstance(layer, nn.Conv2d):\n",
    "                    current_channels = layer.out_channels\n",
    "                continue\n",
    "\n",
    "            vals = {\n",
    "                'kernel_size': layer.kernel_size if isinstance(layer.kernel_size, int) else layer.kernel_size[0],\n",
    "                'stride': layer.stride if isinstance(layer.stride, int) else layer.stride[0],\n",
    "                'padding': layer.padding if isinstance(layer.padding, int) else layer.padding[0],\n",
    "                'dilation': layer.dilation if isinstance(layer.dilation, int) else layer.dilation[0]\n",
    "            }\n",
    "            current_dim = (current_dim + 2*vals['padding'] - vals['dilation']*(vals['kernel_size'])) // vals['stride'] + 1\n",
    "\n",
    "        if isinstance(layer, nn.Conv2d):\n",
    "            current_channels = layer.out_channels\n",
    "\n",
    "        if isinstance(layer, nn.Linear):\n",
    "            if brain:\n",
    "                return current_dim, current_channels\n",
    "            else:\n",
    "                return current_dim * current_dim * current_channels\n",
    "        \n",
    "    raise ValueError(\"No linear layer found in layers! Why are you even asking me?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the CNN\n",
    "\n",
    "*Putting it all together, we should be able to train the CNN*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(train_set, batch_size=16, shuffle=True, collate_fn=collate_fn)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_set, batch_size=16, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "in_channels = next(iter(train_dataloader))[0].shape[1]\n",
    "in_width_height = next(iter(train_dataloader))[0].shape[-1]\n",
    "\n",
    "# Make a dummy model to find out the size before the first linear layer\n",
    "CNN_model = CNN(num_classes=10, in_channels=in_channels, lr=0.001)\n",
    "feats_fore_linear = get_dim_before_first_linear(CNN_model.layers, in_width_height, in_channels, brain=False)\n",
    "\n",
    "# Now make true model when we know how many features we have before the first linear layer\n",
    "CNN_model = CNN(num_classes=10, in_channels=in_channels, features_fore_linear=feats_fore_linear, lr=0.001) \n",
    "\n",
    "train_epochs = 5\n",
    "train_accs, test_accs = CNN_model.train_model(train_dataloader, epochs=train_epochs,  val_dataloader=test_dataloader)\n",
    "\n",
    "# plot train and test accuracies\n",
    "plt.plot(range(train_epochs), train_accs, label='Train')\n",
    "plt.plot(range(train_epochs), test_accs, label='Test')\n",
    "\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Train and Test Accuracy over epochs, CNN model')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.1\n",
    "\n",
    "**1. Reason a bit about why the training and test accuracy look as they do? One question could be: Why is the training accuracy lower than the test accuracy for the first epoch?**\n",
    "\n",
    "$\\dots$\n",
    "\n",
    "**2. Make the network also save the average losses over each epoch as it trains. What does the loss after each epoch tell you about the state of its training?**\n",
    "\n",
    "$\\dots$\n",
    "\n",
    "**3. Can you use the training loss as a measure for the performance of your model?**\n",
    "\n",
    "$\\dots$\n",
    "\n",
    "**4. Try changing some of the hyperparameters of the network (layers, epochs, learning rate, etc.), are you able to achieve a better test accuracy?**\n",
    "\n",
    "**5. Try changing some other hyperparameters, are you able to make the network *overfit?* Reason about why or why your are not able to.**\n",
    "\n",
    "$\\dots$\n",
    "\n",
    "**6. Look into the torch documentatation for [saving and loading models](https://pytorch.org/tutorials/beginner/saving_loading_models.html), try to complete the functions for saving and loading your model resepectively**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining a feedforward neural network (FFNN) class\n",
    "\n",
    "*Here, we don't need to reinvent the wheel, so we just subclass from our existing CNN model to inherit all useful attributes*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFNN(CNN):\n",
    "    def __init__(self, in_features, num_classes, lr=0.001):\n",
    "        # We have to give it dummy values, otherwise intialization will not work\n",
    "        super().__init__(num_classes=num_classes, in_channels=1, lr=0.001)\n",
    "        \n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(in_features=in_features, out_features=32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=32, out_features=64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=64, out_features=128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=128, out_features=64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=64, out_features=num_classes)\n",
    "        ).to(device)\n",
    "\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.optim = torch.optim.Adam(self.layers.parameters(), lr=lr)\n",
    "            \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.flatten(start_dim=1)\n",
    "        # Super unucesseary, but wanted to show that you can call the parent class' forward function\n",
    "        return super().forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(train_set, batch_size=16, shuffle=True, collate_fn=collate_fn)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_set, batch_size=16, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "# Num in_channels really shouldn't be above 1, the FFNN probably can't handle that well\n",
    "num_in_channels = next(iter(train_dataloader))[0].shape[1]\n",
    "in_width_height = next(iter(train_dataloader))[0].shape[-1]\n",
    "\n",
    "FFNN_model = FFNN(in_features=num_in_channels*in_width_height**2,num_classes=10, lr=0.001)\n",
    "\n",
    "train_epochs = 5\n",
    "train_accs, test_accs = FFNN_model.train_model(train_dataloader, epochs=train_epochs, val_dataloader=test_dataloader)\n",
    "\n",
    "# plot train and test accuracies\n",
    "plt.plot(range(train_epochs), train_accs, label='Train')\n",
    "plt.plot(range(train_epochs), test_accs, label='Test')\n",
    "\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Train and Test Accuracy over epochs, FFNN model')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2.2\n",
    "\n",
    "**1. Test the FFNN model for both the CIFAR10 and the MNIST dataset. There should be a *huge* difference in test accuracy when using CIFAR10 compared to MNIST. Explain this gap, and why this gap is comparatively smaller for the CNN model**\n",
    "\n",
    "$\\dots$\n",
    "\n",
    "**2. It should be obvious, that the FFNN model is inferior in terms of test accuracy for this particular task... Does it have any advantages over the CNN?**\n",
    "\n",
    "$\\dots$\n",
    "\n",
    "**3. Try changing the layer structure of your FFNN to see if you can achieve a similar or better test accuracy on CIFAR10 than the CNN. Alternatively, try to see how much you need to hamstring the layers of the CNN to pull it down to the performance level of the FFNN.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Displaying the output of the convlutional layers\n",
    "\n",
    "*Sometimes it is interesting to actually examine how the convolutions in the CNN actually look, to reason about what the model learns. We do this by using Torch's 'forward hooks' - functions that you can apply to layers, that will then be called after the layer performs a forward pass. This can be useful for a bunch of different things, including adding BatchNorm to your model... We just use it to capture the output of each layer*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.4\n",
    "\n",
    "*The below code plots the intermediary outputs of the kernels of either an untrained CNN model, or your trained model. We can examine this output to get an idea of the kind of feature extraction the convolutional layers actually perform.*\n",
    "\n",
    "**1. Consider the intermediary kernel outputs of both a trained and an untrained CNN model, how do they differ?**\n",
    "\n",
    "$\\dots$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model = False\n",
    "\n",
    "if trained_model is False:\n",
    "    # Initialize a dummy untrained CNN model to compare activations with\n",
    "    in_channels = next(iter(train_dataloader))[0].shape[1]\n",
    "    in_width_height = next(iter(train_dataloader))[0].shape[-1]\n",
    "    feats_fore_linear = get_dim_before_first_linear(CNN_model.layers, in_width_height, in_channels, brain=False)\n",
    "    model = CNN(num_classes=10, in_channels=in_channels, features_fore_linear=feats_fore_linear, lr=0.001) \n",
    "\n",
    "else:\n",
    "    # Otherwise use a trained model\n",
    "    model = CNN_model\n",
    "\n",
    "# Dict to hold network activations\n",
    "activations = {}\n",
    "\n",
    "# Hook function, is a type of wrapper function that is then called whenever layer registered with it is run\n",
    "def forward_hook(layer_name, capture_dict):\n",
    "    def hook(module, input, output):\n",
    "        capture_dict[layer_name] = output\n",
    "    return hook\n",
    "\n",
    "# Register hooks for all Conv2d layers model\n",
    "for name, layer in model.named_modules():\n",
    "    if isinstance(layer, nn.Conv2d):\n",
    "        layer.register_forward_hook(forward_hook(name, activations))\n",
    "\n",
    "# Get just a single example from the test dataloader\n",
    "input_image, input_label = next(iter(test_dataloader))\n",
    "input_image, input_label = input_image[0].unsqueeze(0).to('cpu'), input_label[0].unsqueeze(0).to('cpu').numpy()[0]\n",
    "\n",
    "# Plot the image before convolutions\n",
    "if dataset == 'mnist':\n",
    "    plt.imshow(input_image.squeeze().numpy(), cmap='gray')\n",
    "    plt.title(f'Random image, label: {input_label}')\n",
    "\n",
    "elif dataset == 'cifar10':\n",
    "    plt.imshow(input_image.squeeze().transpose(0, 2).transpose(0, 1).numpy())\n",
    "    plt.title(f'Random image, label: {cifar10_classes[input_label]}')\n",
    "plt.axis('off')  # Hide axis\n",
    "plt.show()\n",
    "\n",
    "# Forward pass - Use torch.no_grad() to not store gradients, since we are only interested in activations\n",
    "with torch.no_grad():\n",
    "    print(input_image.shape)\n",
    "    output = model(input_image.to(device))\n",
    "\n",
    "# Display the captured activations\n",
    "for layer_name, activation in activations.items():\n",
    "    print(f\"Layer: {layer_name}, Activation Shape: {activation.shape}\")\n",
    "\n",
    "fig, axes = plt.subplots(len(activations.items()), 4, figsize=(12, 6))\n",
    "\n",
    "for i, (layer_name, activation) in enumerate(activations.items()):\n",
    "    for j in range(4):\n",
    "        axes[i, j].imshow(activation[0, j, ...].to('cpu'), cmap='gray')\n",
    "        axes[i, j].axis('off') \n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adv-ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
